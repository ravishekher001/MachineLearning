{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Numerous groups have applied a variety of deep learning techniques to computer vision problems in highway perception scenarios. In this paper, we presented a number of empirical evaluations of recent deep learning advances. Computer vision, combined with deep learning, has the potential to bring about a relatively inexpensive, robust solution to autonomous driving. To prepare deep learning for industry uptake and practical applications, neural networks will require large data sets that represent all possible driving environments and scenarios. We collect a large data set of highway data and apply deep learning and computer vision algorithms to problems such as car and lane detection. We show how existing convolutional neural networks (CNNs) can be used to perform lane and vehicle detection while running at frame rates required for a real-time system. Our results lend credence to the hypothesis that deep learning holds promise for autonomous driving.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between highways and urban roads\n",
    "Among the different environments for self-driving cars, highway and urban roads are on opposite ends of the spectrum. In general, highways tend to be more predictable and orderly, with road surfaces typically well- maintained and lanes well-marked. In contrast, residential or urban driving environments feature a much higher degree of unpredictability with many generic objects, inconsistent lane- markings, and elaborate traffic flow patterns.\n",
    "\n",
    "### Autonomous driving on highways in regular cars\n",
    "The relative regularity and structure of highways has facilitated some of the first practical applications of autonomous driving technology. Many automakers have begun pursuing highway auto-pilot solutions designed to mitigate driver stress and fatigue and to provide additional safety features; for example, certain advanced-driver assistance systems (ADAS) can both keep cars within their lane and perform front-view car detection.\n",
    "\n",
    "### Users still liable to put their hands on the steering wheel\n",
    "Currently, the human drivers retain liability and, as such, must keep their hands on the steering wheel and prepare to control the vehicle in the event of any unexpected obstacle or catastrophic incident. \n",
    "\n",
    "### Auto-pilot systems vs Self-Driving cars\n",
    "Financial considerations contribute to a substantial performance gap between commercially available auto-pilot systems and fully self-driving cars developed by Google and others. Namely, today’s self-driving cars are equipped with expensive but critical sensors, such as LIDAR, radar and high-precision GPS coupled with highly detailed maps.\n",
    "\n",
    "### Role of sensors in autonomous vehicles\n",
    "In today’s production-grade autonomous vehicles, critical sensors include radar, sonar, and cameras. Long-range vehicle detection typically requires radar, while nearby car detection can be solved with sonar.\n",
    "\n",
    "### Enter Computer Vision\n",
    "Computer vision can play an important a role in lane detection as well as redundant object detection at moderate distances.\n",
    "\n",
    "### Why can't we use Radars instead of Computer Vision ?\n",
    "Radar works reasonably well for detecting vehicles, but has difficulty distinguishing between different metal objects and thus can register false positives on objects such as tin cans. Also, radar provides little orientation information and has a higher variance on the lateral position of objects, making the localization difficult on sharp bends. The utility of sonar is both compromised at high speeds and, even at slow speeds, is limited to a working distance of about 2 meters. Compared to sonar and radar, cameras generate a richer set of features at a fraction of the cost.\n",
    "\n",
    "### Just Computer Vision is not enough !!\n",
    "By advancing computer vision, cameras could serve as a reliable redundant sensor for autonomous driving. Despite its potential, computer vision has yet to assume a significant role in today’s self- driving cars. Classic computer vision techniques simply have not provided the robustness required for production grade automotives; these techniques require intensive hand engineer- ing, road modeling, and special case handling. Considering the seemingly infinite number of specific driving situations, environments, and unexpected obstacles, the task of scaling classic computer vision to robust, human-level performance would prove monumental and is likely to be unrealistic.\n",
    "\n",
    "### Enter Deep Learning\n",
    "Deep learning, or neural networks, represents an alternative approach to computer vision. It shows considerable promise as a solution to the shortcomings of classic computer vision. \n",
    "\n",
    "- Recent progress in the field has advanced the feasibility of deep learning applications to solve complex, real-world problems; industry has responded by increasing uptake of such technology.\n",
    "- Deep learning is data centric, requiring heavy computation but minimal hand-engineering.\n",
    "- In the last few years, an increase in available storage and compute capabilities have enabled deep learning to achieve success in supervised perception tasks, such as image detection.\n",
    "- A neural network, after training for days or even weeks on a large data set, can be capable of inference in real-time with a model size that is no larger than a few hundred MB (Reference 1)\n",
    "\n",
    "### Make use of existing sensors and integrate them with cameras \n",
    "By using expensive existing sensors which are currently used for self-driving applications, such as LIDAR and mm- accurate GPS, and calibrating them with cameras, we can create a video data set containing labeled lane-markings and annotated vehicles with location and relative speed. *By building a labeled data set in all types of driving situations (rain, snow, night, day, etc.), we can evaluate neural networks on this data to determine if it is robust in every driving environment and situation for which we have training data.*\n",
    "\n",
    "### Goal of this paper\n",
    "In this paper, we detail empirical evaluation on the data set we collect. In addition, we explain the neural network that we applied for detecting lanes and cars.\n",
    "\n",
    "\n",
    "\n",
    "Reference 1: Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. ”Imagenet classification with deep convolutional neural networks.” Advances in neural information processing systems. 2012.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper: Speed/accuracy trade-offs for modern convolutional object detectors\n",
    "\n",
    "A lot of progress has been made in recent years on object detection due to the use of convolutional neural networks (CNNs). Modern object detectors based on these networks — such as Faster R-CNN [31], R-FCN [6], Multibox [40], SSD [26] and YOLO [29] — are now good enough to be deployed in consumer products (e.g., Google Photos, Pin- terest Visual Search) and some have been shown to be fast enough to be run on mobile devices.\n",
    "\n",
    "One of the key requirements of the use of these algorithms in self-driving cars is real-time performance. \n",
    "\n",
    "Methods that win competitions using ensemble methods are actually not so practical. For instance, \n",
    "\n",
    "`While the methods that win competitions, such as the COCO challenge [25], are optimized for accuracy, they often rely on model ensembling and multicrop methods which are too slow for practical usage.`\n",
    "\n",
    "Unfortunately, only a small subset of papers (e.g., R- FCN [6], SSD [26] YOLO [29]) discuss running time in any detail. Furthermore, these papers typically only state that they achieve some frame-rate, but do not give a full picture of the speed/accuracy trade-off, which depends on many other factors, such as which feature extractor is used, input image sizes, etc.\n",
    "\n",
    "`In this paper, we seek to explore the speed/accuracy trade-off of modern detection systems in an exhaustive and fair way.` \n",
    "\n",
    "We primarily investigate single- model/single-pass detectors, by which we mean models that do not use ensembling, multi-crop methods, or other “tricks” such as horizontal flipping. In other words, we only pass a single image through a single network.\n",
    "\n",
    "we have created implementations of the Faster R-CNN, R-FCN and SSD meta-architectures, which at a high level consist of a single convolutional network, trained with a mixed regression and classification objective, and use sliding window style predictions.\n",
    "\n",
    "\n",
    "\n",
    "## References:\n",
    "\n",
    "\n",
    "R-FCN:\n",
    "[6] J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection via region-based fully convolutional networks. arXiv preprint arXiv:1605.06409, 2016. 1, 2, 3, 4, 5, 6\n",
    "\n",
    "SSD:\n",
    "[26] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.- Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector. In European Conference on Computer Vision, pages 21–37. Springer, 2016. 1, 2, 3, 4, 5, 6, 7\n",
    "\n",
    "YOLO:\n",
    "[29]: J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You only look once: Unified, real-time object detection. arXiv preprint arXiv:1506.02640, 2015. 1, 3\n",
    "\n",
    "R-CNN Paper:\n",
    "[31] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91–99, 2015. 1, 2, 3, 4, 5, 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper:  You Only Look Once: Unified, Real-Time Object Detection\n",
    "\n",
    "Fast, accurate algorithms for object detection would allow computers to drive cars without specialized sensors, enable assistive devices to convey real-time scene information to human users, and unlock the potential for general purpose, responsive robotic systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many algorithms exist for object and lane detection.\n",
    "\n",
    "Many object detection \n",
    "\n",
    "Many object and lane detection algorithms have been developed in the recent years which are based on Convolutional Neural Networks. An emperical evaluation of the performance of these algorithms for solving object and lane detection problems in the context of self-driving cars has not been attempted. \n",
    "\n",
    "While each algorithm has individually presented its speed and accuracy metrics, what is lacking is a thorough comparision of these algorithms. \n",
    "\n",
    "Dai et al, presented a region-based fully convolutional network for object detection and reported that they achieved a an accuracy close to Faster R-CNN but with better training time. \n",
    "\n",
    "\n",
    "It is important to note that each of these algorightms approach the problem object and lane detection in a unique manner. Our research is primarily focused on evaluating these algorithms on two key metrics: speed and accuracy. \n",
    "\n",
    "\n",
    "We have presented RPNs for efficient and accurate region proposal generation. By sharing convolutional features with the down-stream detection network, the region proposal step is nearly cost-free. Our method enables a unified, deep-learning-based object detection system to run at 5-17 fps. The learned RPN also improves region proposal quality and thus the overall object detection accuracy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dai et al, presented a region-based fully convolutional network for object detection and reported that they achieved a an accuracy close to Faster R-CNN but with better training time. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A small subset of papers have attempted to discuss the running time in any detail.\n",
    "\n",
    "\n",
    "\n",
    "Most of the recent research done in \n",
    "\n",
    "\n",
    "A small subset of papers have attempted to discuss the running time in any detail. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An emperical evaluation of deeplearning algorithms on Highway driving conditions was conducted by []. In this work, the research team used regular CNNs for vehicle and lane detection on Highways. By using Camera, Lidar, Radar, and GPS they built a highway dataset consisting of 17 thousand image frames with vehicle bounding boxes and over 616 thousand image frames with lane annotations. Using this dataset, they trained a CNN and showed that regular CNNs are capable of detecting vehicles and cars in a single forward pass.\n",
    "\n",
    "\n",
    "By using Camera, Lidar, Radar, and GPS we built a highway data set consisting of 17 thousand image frames with vehicle bounding boxes and over 616 thousand image frames with lane annotations. We then trained on this data using a CNN architecture capable of detecting all lanes and cars in a single forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another challenge with comparing these algorithms is finding a way to\n",
    "\n",
    "Finding a way to do an apples-to-apples comparison of these algorithms is the biggest challenge. For instance, some algorithms use different base feature extractors like VGG or Residual Networks, while some others work well with certain image resolutions and yet some others have been specifically trained on certain hardware and software platforms. One research group attempted to address this problem by using a unified implementation of Faster R-CNN, R-FCN and SSD systems, which they call as \"meta-architecutes\". The idea behind using a unified approach is that it is impractical to compare every recently proposed detection system. By grouping alogirthms based on common set of architecutes, it is possible to compare many algorithms that share similar characteristics. In particular, the group of researchers have created implementations of the Faster R-CNN, R-FCN and SSD meta-architectures, which at a high level consist of a single convolutional network, trained with a mixed regression and classification objective, and use sliding window style predictions. The results show that using fewer proposals for Faster R-CNN can speed it up significantly without a big loss in accuracy, making it competitive with its faster cousins, SSD and RFCN. They also showed that SSDs performance is less sensitive to the quality of the feature extractor than Faster R-CNN and R-FCN. \n",
    "\n",
    "Liu et al. introduced SSD, a single-shot detector for multiple categories that is faster than the previous state-of-the-art for single shot detectors (YOLO), and significantly more accurate, in fact as accurate as slower techniques that perform explicit region proposals and pooling (including Faster R-CNN). Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300 × 300 input, SSD achieves 74.3% mAP1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9% mAP, outperforming a comparable state-of-the-art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size.\n",
    "\n",
    "\n",
    "Experiments  include timing and accuracy analysis on models with varying input size evaluated on PASCAL VOC, COCO, and ILSVRC and are compared to a range of recent state-of-the-art approaches.\n",
    "\n",
    "\n",
    " And we identify sweet spots on the accuracy/speed trade-off curve where gains in accuracy are only possible by sac- rificing speed (within the family of detectors presented here).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Unfortunately, only a small subset of papers (e.g., R- FCN [6], SSD [26] YOLO [29]) discuss running time in any detail. Furthermore, these papers typically only state that they achieve some frame-rate, but do not give a full picture of the speed/accuracy trade-off, which depends on many other factors, such as which feature extractor is used, input image sizes, etc.\n",
    "\n",
    "\n",
    "SSD are another class of algorithms \n",
    "\n",
    "Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300 × 300 input, SSD achieves 74.3% mAP1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9% mAP, outperforming a compa- rable state-of-the-art Faster R-CNN model. Compared to other single stage meth- ods, SSD has much better accuracy even with a smaller input image size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Shot Detector has good reference papers\n",
    "\n",
    "SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[19] Huval,B.,Wang,T.,Tandon,S.,et al.(2015).An empirical evaluation of deep learning on highway driving. arXiv preprint arXiv:1504.01716.\n",
    "\n",
    "[20] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.- Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector. In European Conference on Computer Vision, pages 21–37. Springer, 2016. 1, 2, 3, 4, 5, 6, 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
